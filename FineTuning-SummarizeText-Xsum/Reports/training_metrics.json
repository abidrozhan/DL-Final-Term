{
  "model_configuration": {
    "base_model": "microsoft/phi-2",
    "architecture": "decoder-only transformer",
    "total_parameters": "2.7B",
    "trainable_parameters": "~4.2M",
    "trainable_percentage": 0.16,
    "quantization": {
      "type": "4-bit (QLoRA)",
      "quant_type": "nf4",
      "compute_dtype": "float16",
      "double_quantization": true
    },
    "lora_config": {
      "rank": 16,
      "lora_alpha": 32,
      "lora_dropout": 0.05,
      "target_modules": [
        "q_proj",
        "k_proj",
        "v_proj",
        "dense",
        "fc1",
        "fc2"
      ],
      "bias": "none",
      "task_type": "CAUSAL_LM"
    }
  },
  "training_configuration": {
    "dataset": {
      "name": "XSum",
      "train_samples": 2000,
      "validation_samples": 500,
      "full_dataset_size": 204045,
      "subset_reason": "Resource constraints (Google Colab)"
    },
    "hyperparameters": {
      "num_epochs": 3,
      "per_device_train_batch_size": 2,
      "per_device_eval_batch_size": 2,
      "gradient_accumulation_steps": 8,
      "effective_batch_size": 16,
      "learning_rate": 0.0002,
      "lr_scheduler_type": "cosine",
      "warmup_steps": 100,
      "optimizer": "paged_adamw_8bit",
      "weight_decay": 0.01,
      "max_grad_norm": 1.0,
      "fp16": true,
      "seed": 42
    },
    "data_configuration": {
      "max_sequence_length": 1126,
      "tokenizer": "microsoft/phi-2",
      "vocab_size": 50257,
      "padding": "dynamic",
      "truncation": true
    }
  },
  "training_results": {
    "final_metrics": {
      "train_loss": 2.2464,
      "eval_loss": 2.2074,
      "perplexity": 9.09
    },
    "training_progress": {
      "total_steps": "~375 steps (estimated)",
      "eval_strategy": "steps",
      "eval_steps": 100,
      "save_steps": 100
    },
    "convergence": {
      "status": "Partially converged",
      "observation": "Loss still decreasing at end of training",
      "recommendation": "Increase epochs to 5-10"
    }
  },
  "resource_usage": {
    "hardware": "Google Colab T4 GPU",
    "vram": "~15 GB",
    "training_time": "Estimated 1-2 hours",
    "memory_efficiency": "99.84% reduction in trainable parameters"
  }
}